{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from typing import Any, Dict, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to cover in the 2nd article: \n",
    "1. Pipeline\n",
    "    * preprocessing that need to be trained\n",
    "    * model pipeline with switchable preprocessor\n",
    "    * model explanation & visualization\n",
    "2. Algorithm agnostic\n",
    "    * the pipeline should be able to easily switch between algorithms & preprocessing logics\n",
    "    * some complexities might need to be built-in for some component of the pipeline (say explanation) to ensure uniform API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "1. The minimalist algorithm agnostic pipeline (Done)\n",
    "    * ~~static preprocessing~~\n",
    "    * ~~accept any model~~\n",
    "    * ~~demo different algorithms~~\n",
    "2. Pipeline with custom preprocesser (Done)\n",
    "    * ~~custom preprocesser 1 for numeric features~~\n",
    "    * ~~demo the pipeline~~\n",
    "    * ~~customer preprocesser 2 that can handle categorical features too~~\n",
    "    * ~~demo using the same pipeline but calling the advanced preprocesser~~\n",
    "3. Add explainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Minimalist Algorithm Agnostic Pipeline\n",
    "\n",
    "* minimal preprocessing\n",
    "* algorithm agnostic: can be used to experiment with any sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_PIPELINE(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    ML_PIPELINE is an implementation of an MLflow Python model that can be used with any \n",
    "    scikit-learn compatible model (e.g., XGBoost, LightGBM, etc.).\n",
    "    This class includes methods for preprocessing input data, training the model, \n",
    "    and making predictions.\n",
    "\n",
    "    Attributes:\n",
    "        model (BaseEstimator or None): A scikit-learn compatible model instance (initialized as None).\n",
    "        config (Any or None): Optional configuration for the model (initialized as None).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: BaseEstimator = None, config: Any = None):\n",
    "        \"\"\"\n",
    "        Initialize the ML_PIPELINE with the given model and optional configuration.\n",
    "\n",
    "        Parameters:\n",
    "            model (BaseEstimator, optional): A scikit-learn compatible model (e.g., LightGBM, XGBoost).\n",
    "            config (Any, optional): Optional configuration for the model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "    def preprocess_input(self, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess the input data by replacing missing values with 0.\n",
    "\n",
    "        Parameters:\n",
    "            model_input (pd.DataFrame): The input DataFrame to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The preprocessed DataFrame with NaN values replaced by 0.\n",
    "        \"\"\"\n",
    "        processed_input = model_input.copy()\n",
    "        processed_input = processed_input.fillna(0)\n",
    "        return processed_input\n",
    "\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Train the model using the preprocessed training data.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training input data (features).\n",
    "            y_train (pd.Series): The target values for the training data.\n",
    "        \"\"\"\n",
    "        X_train_preprocessed = self.preprocess_input(X_train.copy())\n",
    "        self.model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "    def predict(self, context: Any, model_input: pd.DataFrame) -> Any:\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "            context (Any): Optional context provided by MLflow during the prediction phase.\n",
    "            model_input (pd.DataFrame): The input data to predict on.\n",
    "\n",
    "        Returns:\n",
    "            Any: The predicted probabilities or results.\n",
    "        \"\"\"\n",
    "        processed_model_input = self.preprocess_input(model_input.copy())\n",
    "        return self.model.predict_proba(processed_model_input)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "n_feature = 10\n",
    "n_inform = 4 \n",
    "n_redundant = 3\n",
    "n_samples = 1000\n",
    "X, y = make_classification(n_samples = n_samples, n_features = n_feature, \n",
    "                            n_informative = n_inform, n_redundant=n_redundant, shuffle=False, random_state=12)\n",
    "informative_features_names = [f'inf_{i+1}' for i in range(n_inform)]\n",
    "random_features_names = [f'rand_{i+n_inform+1}' for i in range(n_feature - n_inform)]\n",
    "X = pd.DataFrame(X, columns = informative_features_names + random_features_names)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an XGBClassifier with the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.9664966496649665\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "ml_pipeline = ML_PIPELINE(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "y_prob = ml_pipeline.predict(context = None, model_input = X_test)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"auc:{auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest Classifier with the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.968\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': 100,            \n",
    "    'min_samples_split': 4,         \n",
    "    'min_samples_leaf': 2,          \n",
    "    'max_features': 'sqrt',         \n",
    "    'bootstrap': True,              \n",
    "    'random_state': 32             \n",
    "}\n",
    "model = RandomForestClassifier(**params)\n",
    "ml_pipeline = ML_PIPELINE(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "y_prob = ml_pipeline.predict(context = None, model_input = X_test)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"auc:{auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LightGBM Classifier with the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.972\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'random_state': 42\n",
    "}\n",
    "model = lgb.LGBMClassifier(**params, verbose = -1)\n",
    "ml_pipeline = ML_PIPELINE(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "y_prob = ml_pipeline.predict(context = None, model_input = X_test)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"auc:{auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.972\n"
     ]
    }
   ],
   "source": [
    "# extract the model from the pipeline, you don't need `context` parameter when making the prediction\n",
    "print(f\"auc: {roc_auc_score(y_test, ml_pipeline.model.predict_proba(X_test)[:,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve any particular parameters\n",
    "ml_pipeline.model.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 5,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0,\n",
       " 'min_data_in_leaf': 10,\n",
       " 'verbose': -1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or all the parameter config\n",
    "ml_pipeline.model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is trained on 10 features, including ['inf_1', 'inf_2', 'inf_3', 'inf_4', 'rand_5', 'rand_6', 'rand_7', 'rand_8', 'rand_9', 'rand_10']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model is trained on {ml_pipeline.model._n_features} features, including {ml_pipeline.model.feature_name_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with Preprocessing Logic\n",
    "\n",
    "The above minimalist pipeline has very simple preprocessing logic that is not sufficient for most ML projects. To begin with, at least we need a scalar that can be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Custome Preprocessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom preprocessor that handles scaling and imputation of numeric features.\n",
    "\n",
    "    Attributes:\n",
    "        transformer (Pipeline): Pipeline for scaling and imputing numeric data.\n",
    "        features (List[str]): List of feature names from the input DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the PreProcessor with a placeholder for the transformer pipeline.\n",
    "        \"\"\"\n",
    "        self.transformer = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer on the provided dataset by configuring scaling and imputing on numeric features.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The input features to fit the transformer.\n",
    "            y (pd.Series, optional): Target variable, not used in this method.\n",
    "        \n",
    "        Returns:\n",
    "            PreProcessor: The fitted transformer instance.\n",
    "        \"\"\"\n",
    "        self.features = X.columns.tolist()\n",
    "\n",
    "        if self.features:\n",
    "            self.transformer = Pipeline(steps=[\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('imputer', SimpleImputer(strategy='median'))\n",
    "            ])\n",
    "            self.transformer.fit(X[self.features])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the input data by applying the fitted scaling and imputing pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The input features to transform.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed data with scaled and imputed numeric features.\n",
    "        \"\"\"\n",
    "        X_transformed = pd.DataFrame()\n",
    "\n",
    "        if self.features:\n",
    "            transformed_data = self.transformer.transform(X[self.features])\n",
    "            X_transformed[self.features] = transformed_data\n",
    "\n",
    "        X_transformed.index = X.index\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer on the input data and then transforms it.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The input features to fit and transform.\n",
    "            y (pd.Series, optional): Target variable, not used in this method.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed data.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize the Preprocessor in the PL Pipeline\n",
    "\n",
    "Now let's utilize the preprocessor in the ML pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_PIPELINE(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    ML_PIPELINE is an implementation of an MLflow Python model that can be used with any \n",
    "    scikit-learn compatible model (e.g., XGBoost, LightGBM, etc.).\n",
    "    This class includes methods for preprocessing input data, training the model, \n",
    "    and making predictions.\n",
    "\n",
    "    Attributes:\n",
    "        model (BaseEstimator or None): A scikit-learn compatible model instance (initialized as None).\n",
    "        config (Any or None): Optional configuration for the model (initialized as None).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: BaseEstimator = None, preprocessor = None, config: Any = None):\n",
    "        \"\"\"\n",
    "        Initialize the ML_PIPELINE with the given model and optional configuration.\n",
    "\n",
    "        Parameters:\n",
    "            model (BaseEstimator, optional): A scikit-learn compatible model (e.g., LightGBM, XGBoost).\n",
    "            config (Any, optional): Optional configuration for the model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.config = config\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Train the model using the preprocessed training data.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (pd.DataFrame): The training input data (features).\n",
    "            y_train (pd.Series): The target values for the training data.\n",
    "        \"\"\"\n",
    "        X_train_preprocessed = self.preprocessor.fit_transform(X_train.copy())\n",
    "        self.model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "    def predict(self, context: Any, model_input: pd.DataFrame) -> Any:\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "            context (Any): Optional context provided by MLflow during the prediction phase.\n",
    "            model_input (pd.DataFrame): The input data to predict on.\n",
    "\n",
    "        Returns:\n",
    "            Any: The predicted probabilities or results.\n",
    "        \"\"\"\n",
    "        processed_model_input = self.preprocessor.transform(model_input.copy())\n",
    "        return self.model.predict_proba(processed_model_input)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor()\n",
    "ml_pipeline = ML_PIPELINE(model = model, \n",
    "                          preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.971\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline.fit(X_train, y_train)\n",
    "y_prob = ml_pipeline.predict(context=None, model_input=X_test)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"auc: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easily Migrate to a Different Preprocessor\n",
    "\n",
    "The above preprocessor has some room for improvement. For one, it doesn't handle categorical features. Let's\n",
    "\n",
    "* create a more sophisticated preprocessor with the same API\n",
    "* test switching preprocessor  to the ML pipeline (we shouldn't need to change a single line in the ML Pipeline class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor_v2(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that handles scaling of numeric data and encoding of categorical features.\n",
    "\n",
    "    Attributes:\n",
    "        num_transformer (Pipeline): Pipeline for transforming numerical features, including scaling and imputing.\n",
    "        cat_transformer (Pipeline): Pipeline for transforming categorical features, including imputing and encoding.\n",
    "        transformed_cat_cols (List[str]): List of transformed categorical column names after one-hot encoding.\n",
    "        num_features (List[str]): List of numerical features to be transformed.\n",
    "        cat_features (List[str]): List of categorical features to be transformed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,num_impute_strategy = 'median',cat_impute_strategy = 'most_frequent'):\n",
    "        \"\"\"\n",
    "        Initializes the CustomTransformer with attributes to store the transformers for numeric and categorical data.\n",
    "        \"\"\"\n",
    "        self.num_transformer = None\n",
    "        self.cat_transformer = None\n",
    "        self.num_impute_strategy = num_impute_strategy\n",
    "        self.cat_impute_strategy = cat_impute_strategy\n",
    "        self.transformed_cat_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer on the provided dataset, configuring scaling and encoding as necessary.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): Input features for fitting the transformer.\n",
    "            y (pd.Series, optional): Target variable, not used in this method.\n",
    "        \n",
    "        Returns:\n",
    "            CustomTransformer: The fitted transformer instance.\n",
    "        \"\"\"\n",
    "        self.num_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        self.cat_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        if self.num_features:\n",
    "            self.num_transformer = Pipeline(steps=[\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('imputer', SimpleImputer(strategy=self.num_impute_strategy))\n",
    "            ])\n",
    "            self.num_transformer.fit(X[self.num_features])\n",
    "        \n",
    "        if self.cat_features:\n",
    "            self.cat_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy=self.cat_impute_strategy)),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ])\n",
    "            self.cat_transformer.fit(X[self.cat_features])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def get_transformed_cat_cols(self):\n",
    "        \"\"\"\n",
    "        Generates a list of transformed categorical column names after one-hot encoding.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of one-hot encoded categorical column names.\n",
    "        \"\"\"\n",
    "        cat_cols = []\n",
    "        cats = self.cat_features\n",
    "        cat_values = self.cat_transformer['encoder'].categories_\n",
    "        for cat, values in zip(cats, cat_values):\n",
    "            cat_cols += [f'{cat}_{value}' for value in values]\n",
    "        \n",
    "        return cat_cols\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the input data by applying scaling and encoding based on the fitted transformers.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): Input features to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed data.\n",
    "        \"\"\"\n",
    "        X_transformed = pd.DataFrame()\n",
    "\n",
    "        if self.num_features:\n",
    "            transformed_num_data = self.num_transformer.transform(X[self.num_features])\n",
    "            X_transformed[self.num_features] = transformed_num_data\n",
    "        \n",
    "        if self.cat_features:\n",
    "            transformed_cat_data = self.cat_transformer.transform(X[self.cat_features]).toarray()\n",
    "            self.transformed_cat_cols = self.get_transformed_cat_cols()\n",
    "            transformed_cat_df = pd.DataFrame(transformed_cat_data, columns=self.transformed_cat_cols)\n",
    "            X_transformed = pd.concat([X_transformed, transformed_cat_df], axis=1)\n",
    "        \n",
    "        X_transformed.index = X.index\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer and then applies the transformations to the input data.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): Input features for fitting and transforming.\n",
    "            y (pd.Series, optional): Target variable, not used in this method.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed data.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_v2 = PreProcessor_v2()\n",
    "ml_pipeline = ML_PIPELINE(\n",
    "    model = model, \n",
    "    preprocessor=preprocessor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.971\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline.fit(X_train, y_train)\n",
    "y_prob = ml_pipeline.predict(context= None, model_input=X_test)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"auc:{auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "\n",
    "create a synthatic dataset with categoricla features and some missing, then test \n",
    "1. with original PreProcessor which should result in an error then PreProcessor_v2 which should work fine and \n",
    "2. with different impute strategy in PreProcessor_v2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
